---
layout: post
title: 'Machine Learning (Part 3 of 4): Comparing and Combining Algorithms'
date: 2013-07-10 22:24:02.000000000 -07:00
categories:
- Computer Science
tags:
- machine learning
status: draft
type: post
published: false
meta:
  _edit_last: '25629085'
  _publicize_pending: '1'
  geo_public: '0'
author:
  login: seitad
  email: takeshidanny@gmail.com
  display_name: Daniel Seita
  first_name: ''
  last_name: ''
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p><em>This is the third post in a four-part series about machine learning. You can find the complete series in the <a href="http://seitad.wordpress.com/detailed-directory-of-blog-entries/">archives</a>.</em></p>
<p>Previously, I described pretty much every learning algorithm that I'll be using in this comparison. For reference, here's a list of the ones I'll be using in my analysis.</p>
<ol>
<li>Decision Trees</li>
<li>Naive Bayes</li>
<li>Clustering</li>
<li>K-nearest-neighbor</li>
<li>Artificial Neural Networks</li>
<li>Support Vector Machines</li>
</ol>
<p>This post is split into discussions among algorithms individually, and then how to use <em>ensemble</em> methods to combine learners to achieve better results than either could accomplish on their own.</p>
<p><strong></strong><strong>Part 1: Comparing Individual Learning  Algorithms</strong></p>
<p>First and foremost, I have to repeat this.</p>
<blockquote><p>There is no single best machine learning algorithm for every single task.</p></blockquote>
<p>There, it's that simple. This is one of the biggest reasons why machine learning will not die out. There are simply too many variables in real-life and synthetic data and in computing power/speed that will affect performance, and thus we have to compare advantages and disadvantages. This is why having a fundamental understanding of machine learning algorithms is so crucial, so we can make educated guesses on which learning algorithm is best for our specific purposes.</p>
<p><strong>Part 2: Combining Learning Algorithms</strong></p>
<p>Now these are interesting. What's the rationale behind this, you might ask?</p>
<p><strong>Concluding Thoughts and References</strong></p>
<p>Hopefully I gave some <em>very </em>vague general-purpose rules for deciding which algorithms perform best under certain circumstances. The next post will be deliberately open-ended and focus on <em>research topics</em>, so it will be a lot less standard than what we'd expect from a typical machine learning course.</p>
<p>Now that I've finished most of what I wanted to write about with respect to what would be commonly included in a college-level course, I'd like to provide some references for further reading. I have used all of them at some point.</p>
<ol>
<li>Ethem Alpaydin's Machine Learning Textbook (My course's primary textbook)</li>
<li>Tom Mitchell's Machine Learning Textbook</li>
<li>Stuart Russell and Peter Norvig (Artificial Intelligence)</li>
<li>Andrew Ng's Course Notes (Free)</li>
<li>Introduction to Ensemble, etc.</li>
<li>Anything else from the course curriculum...</li>
</ol>
