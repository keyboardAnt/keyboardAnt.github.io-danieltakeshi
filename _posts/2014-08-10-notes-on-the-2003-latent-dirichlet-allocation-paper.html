---
layout: post
title: Notes on the 2003 Latent Dirichlet Allocation Paper
date: 2014-08-10 19:10:37.000000000 -07:00
categories:
- Computer Science
tags:
- Andrew Ng
- David Blei
- Latent Dirichlet Allocation
- machine learning
- Michael Jordan
- natural language processing
status: draft
type: post
published: false
meta:
  _edit_last: '25629085'
  geo_public: '0'
  _oembed_45eb68947dd6e44e09663f0c4c8d6bcf: '{{unknown}}'
  _oembed_7a0418a4f98b46b74830f6a047bf5f07: '{{unknown}}'
  _oembed_2f2096029a99b6d98f882cb6afad2d12: '{{unknown}}'
  _oembed_27b2949ba886ed14714953fa7490768e: '{{unknown}}'
author:
  login: seitad
  email: takeshidanny@gmail.com
  display_name: Daniel Seita
  first_name: ''
  last_name: ''
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p>If you aren't an avid reader of computer science papers, then you might have missed this 2003 paper on Latent Dirichlet Allocation.</p>
<p>&lt;write a better introduction&gt;</p>
<p>&nbsp;</p>
<p>&lt;Then describe my notes&gt;</p>
<p>**Write an introduction to LDA here** using the paper</p>
<p>-topic modeling</p>
<p>-dirichlet allocation (use LaTeX, introduce probability notation)</p>
<p>-find hidden variables</p>
<p>&nbsp;</p>
<p>This is obviously not going to be able to cover everything. Here is some additional reading you may consider.</p>
<p>1.<a href="http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf"> Latent Dirichlet Allocation</a>, by David Blei, Andrew Ng (hey ... <a href="http://seitad.wordpress.com/2014/07/29/andrew-ngs-machine-learning-class-on-coursera/">remember this guy</a>?), and Michael I. Jordan, in the <em>Journal of Machine Learning Research</em> (2003). This is the original journal paper that describes LDA. Due to the amount of math in it, I would not recommend this as a first reading to someone who is new to LDA.</p>
<p>2. <a href="http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf">Probabilistic Topic Models</a>, by David BleiÂ in <em>Communications of the ACM</em> (2012). This is a less technical version of the LDA journal paper that basically covers the high-level detail. I would suggest reading this paper to start out your LDA studies.</p>
<p>3. <a href="http://obphio.us/pdfs/lda_tutorial.pdf">Latent Dirichlet Allocation: Towards a Deeper Understanding</a>, by Colorado Reed (currently a computer science Ph.D. student at UC Berkeley). This is a writeup he wrote a few years ago, and I found it useful because it explained some of the math in more depth.</p>
<p>4. Edwin Chen wrote an <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">introduction to LDA</a> post on his blog and applied his LDA implementation to <a href="http://blog.echen.me/2011/06/27/topic-modeling-the-sarah-palin-emails/">topic modeling the Sarah Palin emails</a>. These two posts were what really made LDA "click" in my mind after I had slogged through my first read of Blei's journal paper.</p>
<p>5. There are a few more smaller blog posts that deal with small portions of LDA, such as <a href="https://ariddell.org/simple-topic-model.html#fnref:2">this one</a> (which tries to present an introduction to an audience with some background in probability and statistics), this one (which ...), this one (which..), and this one (which...). Of course, this post here probably isn't too different from those, but here, I'm providing links, so hopefully this acts as a portal-like post.</p>
<p>https://ariddell.org/simple-topic-model.html</p>
<p>http://sumidiot.wordpress.com/2012/06/13/lda-from-scratch/</p>
<p>http://confusedlanguagetech.blogspot.com/</p>
<p>http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/</p>
