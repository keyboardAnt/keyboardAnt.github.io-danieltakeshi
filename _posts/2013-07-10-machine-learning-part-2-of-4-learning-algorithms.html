---
layout: post
title: 'Machine Learning (Part 2 of 4): Learning Algorithms'
date: 2013-07-10 22:23:05.000000000 -07:00
categories:
- Computer Science
tags:
- decision tree
- machine learning
status: draft
type: post
published: false
meta:
  _edit_last: '25629085'
  geo_public: '0'
author:
  login: seitad
  email: takeshidanny@gmail.com
  display_name: Daniel Seita
  first_name: ''
  last_name: ''
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p><em>This is the second post in a four-part series about machine learning. You can find the complete series in the <a href="http://seitad.wordpress.com/detailed-directory-of-blog-entries/">archives</a>.</em></p>
<p>In the first post, I gave an overview of the field, provided some definitions, and described what in my opinion is the simplest learning algorithm out there.</p>
<p>In this post, we'll take a more technical view and discuss more powerful learning algorithms. I've tried to organize this post by discussing the least technical / simplest models first and saving the more complicated --- but potentially more powerful --- ones for later. What resulted was a lot of detail, and this post ended up being much longer than I intended.</p>
<p>Keep in mind that while I discuss mainly <em>classification</em>, there are ways to extend these to the second most common type of learning problem, <em>regression</em>.</p>
<p><strong>Decision Trees</strong></p>
<p><!--more--></p>
<p>These are a clear extension of decision stumps. Rather than having just one node, a <a href="http://en.wikipedia.org/wiki/Decision_tree_learning">decision tree</a> can be arbitrarily long. (A <a href="http://en.wikipedia.org/wiki/Tree_(graph_theory)">tree</a> in graph theory, by the way, is defined as an undirected, acyclic graph.) To perform classification, we take our data and "feed" it through the tree starting at the <em>root</em> (i.e. starting) node. At each node, an attribute test is performed. The data then moves on to the next node in the tree depending on the result of the attribute test, and continues doing so until it has reached the end of its branch, where the tree will then suggest a classification for that element. The following image shows an example of a classification tree that divides Titanic passengers into the "survived" and "perished" classes. (Note: "sibsp" is the number of siblings or spouses aboard.)</p>
<p><a href="http://seitad.files.wordpress.com/2013/07/cart_tree_titanic_survivors.png"><img class="aligncenter size-full wp-image-1390" alt="CART_tree_titanic_survivors" src="assets/cart_tree_titanic_survivors.png" width="360" height="340" /></a></p>
<p>Despite having only three attribute comparisons, this tree does a nice job in describing the composition of survivors and perishers in an easy-to-explain fashion. For instance, we can tell immediately that women were more likely to survive compared to men.</p>
<p>A nagging question persists, though: <em>how do we create these trees</em>? Logically, we want to get as distinctive as possible without using too many comparisons. To study this, we'll focus on the <a href="http://en.wikipedia.org/wiki/C4.5_algorithm">C4.5 algorithm</a>, one of many algorithms that forms decision trees. It is a recursive algorithm based on a criteria known as <em>information gain</em>. To understand this, we'll need to know about <em>entropy</em>. Here, we consider entropy to be a <em>measure of purity</em> in a dataset, defined as</p>
<p>$latex Ent(S) = -p_{+} \log_2p_{+} - p_{-}\log_2p_{-}$,</p>
<p>where $latex S$ is a sample of examples/data, and $latex p_{+}$ and $latex p_{-}$ represent the proportion of positive and negative samples in the class, respectively, based on a particular attribute (e.g. for the Titanic example above, we might use gender). For convenience, we define $latex 0 \log 0 = 0$. The entropy function is concave, with its minima at $latex p_{-} = 0$ or $latex p_{-} = 1$, and its maximum at $latex p_{-} = 1/2$. Note that $latex p_{-} + p_{+} = 1$.</p>
<p>A split is said to be <em>pure</em> if after the split, for all branches, all the instances choosing a branch belong to the same class. Clearly, there's no need to perform any further classification along that branch, and entropy will be zero there as one of $latex p_{-}$ or $latex p_{+}$ will be 0 --- with the other being 1 --- and plugging those in the formula for entropy results in zero. (This, by the way, is why we needed $latex 0 \log 0 = 0$.)</p>
<p>With our knowledge of entropy, we can define information gain in terms of entropy. <strong>Information gain, denoted as $latex G(S,A)$, is therefore the expected reduction in entropy of data $latex S$ based on evaluating attribute $latex A$:</strong></p>
<p>$latex G(S,A) = Ent(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} Ent(S_v).$</p>
<p>You can see that the formula takes the difference between the entropy of the current dataset and a summation of entropies based on the smaller datasets that result when considering what values these elements have for attribute $latex A$. That's the general overview of a decision tree learning algorithm: take a dataset, find out which attribute provides the <em>most</em> information gain, split the tree, then continue building it as needed while focusing on smaller subsets of the data.</p>
<p>The formulas can be unintuitive for some, so I suggest manually performing the algorithm on some common, well-known, but <em>small</em> datasets. It can get tedious, so only do this once, and after this, one might consider programming the algorithm. (I did this as part of an assignment.) Professor Tom Mitchell of Carnegie Mellon has some clear <a href="http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/mlbook/ch3.pdf">lecture notes</a> that may be helpful in this regard; look at slides 54 and onwards to see a practice dataset with 14 instances.</p>
<p><strong>Linear Perceptrons and Basic Regression</strong></p>
<p>These are somewhat simple compared to the more advanced multilayer perceptron. They're also related to regression.</p>
<p><strong>Naive Bayes</strong></p>
<p>Why's it called naive, you might ask, First, we need to discuss Bayes' Rule.</p>
<p><strong>k-Nearest-Neighbor</strong></p>
<p>Here's another relatively intuitive one.</p>
<p><strong>Clustering</strong></p>
<p>Here's one that relates to <em>unsupervised learning</em>.</p>
<p><strong>Artificial Neural Networks (aka Multilayer Backpropagation)</strong></p>
<p>Boy, these are intense, and can take a very long time.</p>
<p><strong>Support Vector Machines</strong></p>
<p>I've included these last, for good reason. Many machine learning people view these as often one of the best, if not the best general purpose learning algorithm.</p>
<p><strong>What's Coming Next</strong></p>
<p>Now that we've gone through a detailed overview of some of the most common learning algorithms, it's time to compare them and see some of the common advantages and disadvantages. In particular, I'll make use of published results and my own experience with evaluation methodology. In addition, we'll investigate ways we can <em>combine </em>learning algorithms to perform something better than what could be done with individual learning algorithms.</p>
