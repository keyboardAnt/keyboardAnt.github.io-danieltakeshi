---
layout: post
title: Independent Component Analysis -- A Gentle Introduction
date: 2015-01-03 12:06:25.000000000 -08:00
categories:
- Computer Science
tags:
- algorithms
- Andrew Ng
- gradient
- Independent Component Analysis
- machine learning
- stochastic gradient descent
status: publish
type: post
published: true
meta:
  _edit_last: '25629085'
  geo_public: '0'
  _publicize_pending: '1'
  _wpas_skip_facebook: '1'
  _wpas_skip_google_plus: '1'
  _wpas_skip_twitter: '1'
  _wpas_skip_linkedin: '1'
  _wpas_skip_tumblr: '1'
  _wpas_skip_path: '1'
author:
  login: seitad
  email: takeshidanny@gmail.com
  display_name: Daniel Seita
  first_name: ''
  last_name: ''
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p>In this post, I give a brief introduction to independent component analysis (ICA), a machine learning algorithm useful for a certain niche of problems. It is not as general as, say, regression, which means many introductory machine learning courses won't have time to teach ICA. I first describe the rationale and problem formulation. Then I discuss a common algorithm to solve ICA, courtesy of Bell and Sejnowski.</p>
<p><em><strong>Motivation and Problem Formulation</strong></em></p>
<p>Here's a quick technical overview: the purpose of ICA is to explain some desired non-Gaussian data by figuring out a linear combination of statistically independent components. So what does thatÂ <em>really</em> mean?</p>
<p>This means we have some random variable -- or more commonly, a random <em>vector</em> -- that we observe, and which comes from a combination of different data sources. Consider the canonical cocktail party problem. Here, we have a group of people conversing at some party. There are two microphones stationed in different locations of the party room, and at time indices $latex i = \{1, 2, \ldots \}$, the microphones provide us with voice measurements $latex x_1^{(i)}$ and $latex x_2^{(i)}$, such as amplitudes.</p>
<p>For simplicity, suppose that throughout the entire party, only two people are actually speaking, and that their speech signals are <em>independent</em> of each other (this is crucial). At time index $latex i$, they speak with signals $latex s_1^{(i)}$ and $latex s_2^{(i)}$, respectively. But since the two people are in different locations of the room, the microphones each record signals from a <em>different combination</em> of the two people's voices. The goal of ICA is, given the time series data from the microphones, to figure out the original speakers' speech signals. The combination is assumed to be <em>linear</em> in that</p>
<ul>
<li>$latex x_1^{(i)} = a_{11}s_1^{(i)} + a_{12}s_2^{(i)}$</li>
<li>$latex x_2^{(i)} = a_{21}s_1^{(i)} + a_{22}s_2^{(i)}$</li>
</ul>
<p>for unknown coefficients $latex a_{11},a_{12},a_{21},a_{22}$.</p>
<p>Here's a graphical version, <a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf">from a well-known ICA paper</a>. The following image shows two (unrealistic) wavelength diagrams of two people's voices:</p>
<p><a href="https://seitad.files.wordpress.com/2015/01/ica1.png"><img class="aligncenter size-large wp-image-2195" src="assets/ica1.png?w=460" alt="ica1" width="460" height="186" /></a>The data that is observed from the two microphones is in the following image:</p>
<p><a href="https://seitad.files.wordpress.com/2015/01/ica2.png"><img class="aligncenter size-large wp-image-2196" src="assets/ica2.png?w=460" alt="ica2" width="460" height="191" /></a>The goal is to recover the original people's wavelengths (i.e., the two graphs in the first of the two images I posted) when we are only given the observed data (i.e., the two graphs from the second image). Intuitively, it seems like the first observed wavelength must have come from a microphone closer to the first person, because its shape more closely matches person 1's wavelength. The opposite is true for the second microphone.</p>
<p>More generally, consider having $latex n$ microphones and $latex n$ independent speakers; the numerical equality of microphones and speakers is for simplicity. In matrix form, we can express the ICA problem as $latex x^{(i)} = As^{(i)}$ where $latex A$ is an <em>unknown, square, invertible</em> mixing matrix that does not depend on the time interval. Like the assumptions regarding $latex n$, the invertibility of $latex A$ is to make our problem simple to start. We also know that all $latex x^{(i)}$ and $latex s^{(i)}$ are $latex n$-dimensional <em>random</em> vectors. The goal is to recover the unseen sources $latex s^{(i)}$. To simplify the subsequent notation, I omit the $latex i$ notation, but keep in mind that it's there.</p>
<p>How does the linear combination part I mentioned earlier relate to this problem formulation? When we express problems in $latex x = As$ form, that can be viewed as taking linear combinations of components of $latex s$ along with the appropriate row of $latex A$. For instance, the <em>first component</em> (remember, there are $latex n$ of them) of the vector $latex x$ is the dot product of the first row of $latex A$ and the full vector $latex s$. This is a linear combination of independent source signals $latex s_1,s_2,\ldots,s_n$ with coefficients $latex a_{11},a_{12}, \ldots, a_{1n}$ based on the first row of $latex A$.</p>
<p>Before moving on to an algorithm that can recover the sources, consider the following insights:</p>
<ol>
<li>What happens if we know $latex A$? Then multiply both sides of $latex x = As$ by $latex A^{-1}$ and we are done. Of course, the point is that we <em>don't</em> know $latex A$. It is what computer scientists call a set of <em>latent</em> variables. In fact, one perspective of our problem is that we need to get the optimal $latex A^{-1}$ based on our data.</li>
<li>The following ambiguities regarding $latex A$ will always hold: we cannot determine the variance of the components of $latex s$ (due to scalars canceling out in $latex A$) and we also cannot determine ordering of $latex s$ (due to permutation matrices). Fortunately, these two ambiguities are not problematic in practice.</li>
<li>One additional assumption that ICA needs is that the independent source components $latex s_1, s_2, \ldots, s_n$ are <em>not</em> Gaussian random variables. If they are, then the rotational symmetry of Gaussians means we cannot distinguish among the distributions when analyzing their combinations. This requirement is the same as ensuring that the $latex s$ vector is <em>not</em> multivariate Gaussian.</li>
</ol>
<p>Surprisingly, as long as the source components are non-Gaussian, ICA will typically work well for a range of practical problems! Next, I'd like to discuss <em>how</em> we can "solve" ICA.</p>
<p><em><strong>The Bell and Sejnowski ICA Algorithm<br />
</strong></em></p>
<p>We describe a simple stochastic gradient descent algorithm to learn the parameter $latex A^{-1}$ of the model. To simplify notation, let $latex W = A^{-1}$ so that its rows can be denoted by $latex w_i^\top$. Broadly, the goal is to figure out some way of determining the <em>log-likelihood </em>of the training data that depends on the parameter $latex W$, and then perform updates to iteratively improve our estimated $latex W$. This is how stochastic gradient descent typically works, and the normal case is to take logarithms to make numerical calculations easier to perform. Also, we will assume the data $latex x_i$ are zero-mean, which is fine because we can normally "shift" a distribution to center it at 0.</p>
<p>For ICA, suppose we have $latex m$ time stamps $latex i = \{1, 2, \ldots, m\}$. The log-likelihood of the data is</p>
<p>$latex \ell(W) = \sum_{i=1}^{m} \left( \log |W| + \sum_{j=1}^{n} \log g'(w_j^\top x^{(i)}) \right)$,</p>
<p>where we note the following:</p>
<ol>
<li>$latex |W|$ is the determinant of $latex W$</li>
<li>$latex g'$ is the <em>derivative</em> of the sigmoid function $latex g$ (<em>not </em>the sigmoid function itself!)</li>
</ol>
<p>Let's explain why this formula makes sense. It comes from taking logarithms of the density of $latex x^{(i)}$ at each time stamp. Note that $latex x = As$ so if we let $latex p$ denote the <em>density</em> function of $latex x^{(i)}$, then $latex p(x^{(i)}) = |W| \prod_{j=1}^{n} p_j(w_j^\top x^{(i)})$, where $latex p_j$ is the density of the individual source $latex j$. We can split the product this way due to the independence among the sources, and the $latex w_j$ terms are just (vector) constants so they can be separated as well. For a more detailed overview, see Andrew Ng's lecture notes; in particular, we need the $latex |W|$ term due to the effect of linear transformations.</p>
<p>Unfortunately, we don't know the density of the individual sources, so we approximate them with some "good" density and make them equal to each other. We can do this by taking the derivative of the sigmoid function:</p>
<p><a href="https://seitad.files.wordpress.com/2015/01/logistic-curve.png"><img class="aligncenter size-large wp-image-2209" src="assets/logistic-curve.png?w=460" alt="Logistic-curve" width="460" height="307" /></a>The reason why this works is that the sigmoid function satisfies the properties of a <em>cumulative distribution function</em>, and by differentiating such a function, we get a <em>probability density function</em>. And since it works well in practice (according to Andrew Ng), we might as well use it.</p>
<p>Great, so now that we have the log-likelihood equation, what is the stochastic gradient descent update rule? It is (remember that $latex g$ is the sigmoid function):</p>
<p>$latex W_{t} = W_{t-1} + \alpha \left( \begin{bmatrix} 1-2g(w_1^\top x^{(i)}) \\ 1-2g(w_2^\top x^{(i)}) \\ \vdots \\ 1-2g(w_n^\top x^{(i)}) \end{bmatrix} (x^{(i)})^\top + ((W_{t-1})^\top)^{-1} \right)$,</p>
<p>where $latex \alpha$ is the standard learning rate parameter, and the $latex i$ that we pick for each iteration update varies (ideally sampling from the $latex m$ training data pieces uniformly). Notice that the term in the parentheses is a matrix: we're taking an outer product and then adding another matrix. To get that update rule from the log-likelihood equation, we take the gradient $latex \nabla_W \ell(W)$, though I think we omit the first summation over $latex m$ terms. Matrix calculus can be tricky and one of the best sources I found for learning about this is (surprise) <a href="http://cs229.stanford.edu/section/cs229-linalg.pdf">another one of Andrew Ng's lecture notes</a> (look near the end). It took a while for me to verify but it should work as long as the $latex m$ summation is omitted, i.e., we do this for a fixed $latex x^{(i)}$. To find the correct outer product vectors to use, it may help to use the sigmoid's nice property that $latex g'(x) = g(x) (1-g(x))$. Lastly, don't forget to take the logarithm into account when taking the derivatives. I can post my full gradient calculation later if there's enough demand.</p>
<p>There are whole books written on how to decide when to stop iterating, so I won't get into that. Once it converges, perform $latex s^{(i)} = Wx^{(i)}$ and we are done, assuming we just wanted the $latex s$ vectors at all times.</p>
<p>Well, that's independent component analysis. Remember that this is just one way to solve related problems, and it's probably on the easier side.</p>
<p><em><strong>References and Further Reading<br />
</strong></em></p>
<ol>
<li><a href="http://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf">Independent Component Analysis: Algorithms and Applications</a></li>
<li><a href="http://cs229.stanford.edu/notes/cs229-notes11.pdf">Andrew Ng's lecture notes</a></li>
</ol>
<p>Let me know if you found this article useful.</p>
